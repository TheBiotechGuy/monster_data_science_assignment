{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_Solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheBiotechGuy/monster_data_science_assignment/blob/master/Assignment_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XFI3eJYe-Zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing required libraries\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import math\n",
        "from bson.json_util import dumps\n",
        "from bson import json_util, ObjectId\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3buKo0-e-Zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_json(file_path):\n",
        "  \"\"\"\n",
        "  This function reads the json file using json and bson packages\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  file_path: Path of input json file\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  data: Iterable object over all the json data\n",
        "  \n",
        "  \"\"\"\n",
        "  with open(file_path, encoding='utf-8', errors='ignore') as data_file:\n",
        "    data = json.loads(json_util.dumps(data_file))\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ysl_N74guea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = read_json('searchjson.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cfbV96DlaHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_all_key_combinations_all_rows(data):\n",
        "  \n",
        "  \"\"\"\n",
        "  After reading the json file using read_json function, you might want to look at what all keys each entry has.\n",
        "  \n",
        "  Are these keys same or different in all the rows ?\n",
        "  \n",
        "  This function will return all unique key combinations across all entries in json file\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  \n",
        "  data: Iterable object after reading the json file\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  List of all the the unique key combinations across all entries in json file\n",
        "  \n",
        "  \"\"\"\n",
        "  updated_data = []\n",
        "  first_entry_data_keys = set(eval(data[0]).keys())\n",
        "  difference_keys = []\n",
        "  Kinds_of_rows = []\n",
        "  for entry in data:\n",
        "    try:\n",
        "      entry = eval(entry)\n",
        "      entry_keys = set(entry.keys())\n",
        "      updated_data.append(entry)\n",
        "      if len(entry_keys.difference(first_entry_data_keys)):\n",
        "        Kinds_of_rows.append(entry)\n",
        "        difference_keys.append(entry_keys.difference(first_entry_data_keys))\n",
        "    except:\n",
        "      continue\n",
        "  return Kinds_of_rows, difference_keys, updated_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPYQSg1kng3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def possbible_columns_in_df(data):\n",
        "  \"\"\"\n",
        "  This function returns all possible columns to add in the dataframe. These column values are retrived by combining all the keys present in the json file.\n",
        "  \"\"\"\n",
        "  Kinds_of_rows, difference_keys, updated_data = get_all_key_combinations_all_rows(data)\n",
        "  extra_keys_in_some_entries = set().union(*difference_keys)\n",
        "  common_columns_amongst_all_rows = eval(data[0]).keys()\n",
        "  all_possible_columns = list(set(common_columns_amongst_all_rows).union(extra_keys_in_some_entries))\n",
        "  return all_possible_columns, updated_data\n",
        "\n",
        "\n",
        "all_possible_columns, updated_data = possbible_columns_in_df(data)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8zmqBUJIf23",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgxK20U193jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df = pd.DataFrame(columns = all_possible_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUxm4WD_H3IW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def append_rows_to_df(data_df, data):\n",
        "  \n",
        "  \"\"\"\n",
        "  This function will append all the json entries as rows in the dataframe\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  data_df: Df to append the rows\n",
        "  \n",
        "  data: A list of dictionaries which are to be appended as rows\n",
        "  \n",
        "  Returns:\n",
        "  data_df: Updated datafarme with rows appended\n",
        "  \n",
        "  \"\"\"\n",
        "  data_df = data_df.append(data, ignore_index=True)\n",
        "  return data_df\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6_A7kNcIwr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updated_df = append_rows_to_df(data_df, updated_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBwX-16gK9uF",
        "colab_type": "code",
        "outputId": "69afc34c-3c2b-43f3-aa26-08de3a367fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "updated_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>channel</th>\n",
              "      <th>sort</th>\n",
              "      <th>_id</th>\n",
              "      <th>referer</th>\n",
              "      <th>query</th>\n",
              "      <th>queryall</th>\n",
              "      <th>queryany</th>\n",
              "      <th>expressresume</th>\n",
              "      <th>subuid</th>\n",
              "      <th>searchwithin</th>\n",
              "      <th>search_from</th>\n",
              "      <th>signature</th>\n",
              "      <th>resultcount</th>\n",
              "      <th>queryexcludes</th>\n",
              "      <th>filter</th>\n",
              "      <th>start</th>\n",
              "      <th>filter_service</th>\n",
              "      <th>excludesynonym</th>\n",
              "      <th>mltcount</th>\n",
              "      <th>fl</th>\n",
              "      <th>rows</th>\n",
              "      <th>logtime</th>\n",
              "      <th>highlight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>upt</td>\n",
              "      <td>5cd471b51ce48c8d454bb62c</td>\n",
              "      <td>SBR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'sort': 'upt', 'seekerservices': 'ER1', 'rows...</td>\n",
              "      <td>643048</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Resume Search</td>\n",
              "      <td>b8a69c82819dcb276999275e501470c1</td>\n",
              "      <td>141</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'refine_flag': '1', 'maxSal': 100, 'roles': '...</td>\n",
              "      <td>0</td>\n",
              "      <td>{'cat': '1000,907,1,2,3,786,4,5,6,7,8,9,10,11,...</td>\n",
              "      <td>default</td>\n",
              "      <td>NaN</td>\n",
              "      <td>id,upt</td>\n",
              "      <td>160</td>\n",
              "      <td>2019-05-10 00:00:11</td>\n",
              "      <td>true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>rel</td>\n",
              "      <td>5cd471bbd8228c0a15122ddf</td>\n",
              "      <td>PWR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'sort': 'rel', 'seekerservices': 'ER1', 'rows...</td>\n",
              "      <td>642343</td>\n",
              "      <td>contents</td>\n",
              "      <td>Resume Search</td>\n",
              "      <td>967642e6878e9ca2cf922c8777d37226</td>\n",
              "      <td>35</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'maxSal': 150, 'primskill25': 'OR', 'locCondi...</td>\n",
              "      <td>0</td>\n",
              "      <td>{'cat': '1000,907,1,2,3,786,4,5,6,7,8,9,10,11,...</td>\n",
              "      <td>default</td>\n",
              "      <td>NaN</td>\n",
              "      <td>id,upt</td>\n",
              "      <td>40</td>\n",
              "      <td>2019-05-10 00:00:18</td>\n",
              "      <td>true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>rel</td>\n",
              "      <td>5cd471d357ad8c0d292cba2e</td>\n",
              "      <td>PWR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'sort': 'rel', 'seekerservices': 'ER1', 'rows...</td>\n",
              "      <td>642343</td>\n",
              "      <td>contents</td>\n",
              "      <td>Resume Search</td>\n",
              "      <td>d8a5dfd7f35a420ca26da2cb3ec40eb1</td>\n",
              "      <td>661</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'refine_flag': '1', 'maxSal': 150, 'primskill...</td>\n",
              "      <td>0</td>\n",
              "      <td>{'cat': '1000,907,1,2,3,786,4,5,6,7,8,9,10,11,...</td>\n",
              "      <td>default</td>\n",
              "      <td>NaN</td>\n",
              "      <td>id,upt</td>\n",
              "      <td>40</td>\n",
              "      <td>2019-05-10 00:00:41</td>\n",
              "      <td>true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>rel</td>\n",
              "      <td>5cd471d81ce48c8d454bb62d</td>\n",
              "      <td>PWR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Software Development Manager OR Technical Del...</td>\n",
              "      <td>{'sort': 'rel', 'seekerservices': 'ER6', 'rows...</td>\n",
              "      <td>601634</td>\n",
              "      <td>contents</td>\n",
              "      <td>Resume Search</td>\n",
              "      <td>7efe34da00a06886d25acd38cdea07e3</td>\n",
              "      <td>155</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'primskill25': 'OR', 'locCondition': 'OR', 'n...</td>\n",
              "      <td>0</td>\n",
              "      <td>{'cl': '167,171,249,183,190,254', 'pref': '167...</td>\n",
              "      <td>default</td>\n",
              "      <td>NaN</td>\n",
              "      <td>id,upt</td>\n",
              "      <td>40</td>\n",
              "      <td>2019-05-10 00:00:44</td>\n",
              "      <td>true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>rel</td>\n",
              "      <td>5cd471d96e4d8cf964c152e6</td>\n",
              "      <td>PWR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"HR Business Partner\",</td>\n",
              "      <td>{'sort': 'rel', 'seekerservices': 'ER1', 'rows...</td>\n",
              "      <td>492803</td>\n",
              "      <td>contents</td>\n",
              "      <td>Resume Search</td>\n",
              "      <td>18249bb7ff94e5c9d666833ae0380a33</td>\n",
              "      <td>413</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'refine_flag': '1', 'primskill25': 'OR', 'ref...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>default</td>\n",
              "      <td>NaN</td>\n",
              "      <td>id,upt</td>\n",
              "      <td>40</td>\n",
              "      <td>2019-05-10 00:00:48</td>\n",
              "      <td>true</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  channel sort                       _id  ... rows              logtime highlight\n",
              "0       1  upt  5cd471b51ce48c8d454bb62c  ...  160  2019-05-10 00:00:11      true\n",
              "1       1  rel  5cd471bbd8228c0a15122ddf  ...   40  2019-05-10 00:00:18      true\n",
              "2       1  rel  5cd471d357ad8c0d292cba2e  ...   40  2019-05-10 00:00:41      true\n",
              "3       4  rel  5cd471d81ce48c8d454bb62d  ...   40  2019-05-10 00:00:44      true\n",
              "4       1  rel  5cd471d96e4d8cf964c152e6  ...   40  2019-05-10 00:00:48      true\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2O0LgvO1TGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_multiple(main_string, to_be_replaces, new_sring):\n",
        "  \"\"\"\n",
        "  This function replaces multiple substrings in a text by a replacement string\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  main_string: Text string\n",
        "  \n",
        "  to_be_replaces: List of substrings to be replaced in the text, for example: [\" or \", \" and \"]\n",
        "  \n",
        "  new_sring: \n",
        "  \n",
        "  \"\"\"\n",
        "  # Iterate over the strings to be replaced\n",
        "  for elem in to_be_replaces :\n",
        "    # Check if string is in the main string\n",
        "    if elem in main_string :\n",
        "      # Replace the string\n",
        "      main_string = main_string.replace(elem, new_sring)\n",
        "    \n",
        "  return main_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64_jp-2E5ZTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_column_text(input_df, column_name):\n",
        "  \n",
        "  \"\"\"\n",
        "  This function pre-processes the given column by vectorized operations:\n",
        "  \n",
        "  1. Replace all the characters except alphabets and comma by white space\n",
        "  2. Convert the whole string to lowercase characters\n",
        "  3. Replace ' or ' and ' and ' sub strings in the text by ' , '\n",
        "  4. Split the string by comma character\n",
        "  5. After step 4, you get a list of words for each text, now strip all the whitespaces in each word at start and end\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  df: Dataframe object whose column is to be pre-processed\n",
        "  column_name: Name of the column which is to be pre-processed in the df\n",
        "  \n",
        "  Returns:\n",
        "  df: Updated df with pre-processed column\n",
        "  \n",
        "  \"\"\"\n",
        "  df = input_df.copy()\n",
        "  df[column_name] = df[column_name].apply(lambda x: re.sub('[^,a-zA-Z]', \" \", x) if isinstance(x, str) else \"Not String\")\n",
        "  df[column_name] = df[column_name].apply(lambda x: x.lower())\n",
        "  df[column_name] = df[column_name].apply(lambda x: replace_multiple(x, [' or ', ' and '], \" , \"))\n",
        "  df[column_name] = df[column_name].apply(lambda x: x.split(','))\n",
        "  df[column_name] = df[column_name].apply(lambda x: list(set([y.strip() for y in x])))\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-pdZ0oi5eWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_processed_df = preprocess_column_text(updated_df, 'queryany')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2Qx1BKNEnbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5cbaeaba-534c-412f-ef1b-e9af191e8136"
      },
      "source": [
        "pre_processed_df['queryany'].head(10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                         [not string]\n",
              "1                                         [not string]\n",
              "2                                         [not string]\n",
              "3    [technical delivery manager, software developm...\n",
              "4                              [hr business partner, ]\n",
              "5                                      [, it software]\n",
              "6    [, abaqus, structural analysis, ansys workbenc...\n",
              "7                                         [not string]\n",
              "8    [, abaqus, structural analysis, ansys workbenc...\n",
              "9                [active directory, group policy, dns]\n",
              "Name: queryany, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-cXspDhEqut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_bag_of_words(input_df, column_name):\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  This function creates a set out of list of lists\n",
        "  Parameters\n",
        "  ----------\n",
        "  input_df: Input Dataframe\n",
        "  column_name: Name of the column which as lists as it's elements\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  \n",
        "  combined_set: A union set of all the key words in the given column\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  text_list_of_list = list(pre_processed_df[column_name])\n",
        "  combined_list = list(set().union(*text_list_of_list))\n",
        "  if '' in combined_list:\n",
        "    combined_list.remove('')\n",
        "  \n",
        "  return combined_list\n",
        " \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS1oHMlIPas6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bag_of_words = create_bag_of_words(pre_processed_df,'queryany')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg-boSmyPgOk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d64630c-245a-4780-f86b-d95c0e954bdb"
      },
      "source": [
        "print(\"No of unique skills in all the json data :\", len(bag_of_words))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of unique skills in all the json data : 21532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP6clTwvdVNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sparse_matrix_row(search_list, bag_of_words):\n",
        "  \n",
        "  \"\"\"\n",
        "  This function will give a list with 0 and 1 elements represeting if the word is present in the bag_of_words list\n",
        "  \n",
        "  Parametrs\n",
        "  ---------\n",
        "  search_list: list of elements to search in bag of words\n",
        "  \n",
        "  bag_of_words: list of all combined queries \n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  sparse_list: list containing 0s and 1s representing keyword present or not\n",
        "  \n",
        "  \"\"\"\n",
        "  sparse_list = [0]*len(bag_of_words)\n",
        "  index_list = [bag_of_words.index(x) for x in search_list if x in bag_of_words]\n",
        "  for index in index_list:\n",
        "    sparse_list[index] = 1\n",
        "  return sparse_list\n",
        "    \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzzcsCh9dXaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8987bbd3-329d-49b8-f1d5-a948f2e6c8a7"
      },
      "source": [
        "# Add a column containing rows of sparse matrix\n",
        "pre_processed_df['queryany'][6]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'abaqus',\n",
              " 'structural analysis',\n",
              " 'ansys workbench',\n",
              " 'durability analysis',\n",
              " 'chassis durability']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2gusfyAjMQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pre_processed_df['sparse_matrix_rows'] = pre_processed_df['queryany'].apply(lambda x : create_sparse_matrix_row(x, bag_of_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS8qN2qGvKwx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIVEIIKDy8_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a sparse matrix for all the skills across all rows\n",
        "def create_sparse_matrix(pre_processed_df, col):\n",
        "  \n",
        "  \"\"\"\n",
        "  Creates a sparse matrix using the queryany column\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  pre_processed_df: Pre-processed data frame\n",
        "  \n",
        "  col: column name\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  sparse_df: Sparse matrix of word count\n",
        "  \n",
        "  \"\"\"\n",
        "  row_list =[]\n",
        "  i=0\n",
        "  for word_list in list(pre_processed_df[col]):\n",
        "    sparse_row = create_sparse_matrix_row(word_list, bag_of_words)\n",
        "    row_list.append(sparse_row)\n",
        "  \n",
        "    i = i+1\n",
        "    if i==10000: # Only considering the first 10,000 entries in the json file due to memory constraints, top ten skills will be judged based on only these entries\n",
        "      break\n",
        "  sparse_df = pd.DataFrame(row_list, columns = bag_of_words)\n",
        "  return sparse_df\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dg68B2OoUFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sparse_df = create_sparse_matrix(pre_processed_df, 'queryany')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOCrhgwI-OyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_skills_dict(sparse_df):\n",
        "  \"\"\"\n",
        "  Create a dictionary of all the skills and their frequency (sum of 0s and 1s for that particular skill across all rows)\n",
        "  Parameters\n",
        "  ----------\n",
        "  sparse_df: sparse matrix of frequency of words\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  skills_dictionary_with_frequencies = {}\n",
        "  for col in sparse_df:\n",
        "    if not col==\"not string\":\n",
        "      skills_dictionary_with_frequencies[col] = sparse_df[col].sum()\n",
        "  return skills_dictionary_with_frequencies\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuC9qOTyqOKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "skills_dictionary_with_frequencies = create_skills_dict(sparse_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYgTpIzPBWkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_n_skills_dict(skills_dictionary_with_frequencies, n_skills):\n",
        "  \n",
        "  \"\"\"\n",
        "  Returns the top n dict key-pair based on the frequency value\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  skills_dictionary_with_frequencies: dict with skill and frequency key-value pair\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  top_10_skills: No of top skills \n",
        "  \n",
        "  \"\"\"\n",
        "  counter = Counter(skills_dictionary_with_frequencies)  \n",
        "  top_n_skills = counter.most_common(n_skills)\n",
        "  return top_n_skills\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spQKRIzHESPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "99b86dd5-6fa5-43e5-8e90-f62a2a89171a"
      },
      "source": [
        "top_10_skills = get_top_n_skills_dict(skills_dictionary_with_frequencies, 10)\n",
        "print(\"Top 10 skills are as following: \\n\")\n",
        "for i in top_10_skills: \n",
        "    print(i[0],\" :\",i[1],\" \")"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 skills are as following: \n",
            "\n",
            "java  : 483  \n",
            "j ee  : 216  \n",
            "c  : 188  \n",
            "net  : 185  \n",
            "customer service  : 177  \n",
            "technical support  : 160  \n",
            "testing  : 159  \n",
            "sales  : 150  \n",
            "customer support  : 143  \n",
            "marketing  : 134  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKGCFTLyEUpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}